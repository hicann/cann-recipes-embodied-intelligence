diff -ruN alpamayo/pyproject.toml alpamayo_2/pyproject.toml
--- alpamayo/pyproject.toml	2026-02-10 19:25:45.991964500 +0800
+++ alpamayo_2/pyproject.toml	2026-02-10 19:56:40.853271100 +0800
@@ -1,7 +1,7 @@
 [project]
 name = "alpamayo_r1"
 version = "0.1.0"
-requires-python = "==3.12.*"
+requires-python = ">=3.11.13"
 dependencies = [
   "accelerate>=1.12.0",
   "av>=16.0.1",
@@ -9,12 +9,23 @@
   "hydra-colorlog>=1.2.0",
   "hydra-core>=1.3.2",
   "pandas>=2.3.3",
-  "physical_ai_av>=0.1.0",
   "pillow>=12.0.0",
   "torch==2.8.0",
-  "torchvision>=0.23.0",
+  "torch-npu==2.8.0",
+  "torchvision==0.23.0",
   "transformers==4.57.1",
-  "flash-attn>=2.8.3",
+  "setuptools==80.10.2",
+  "numpy==2.4.2",
+  "decorator==5.2.1",
+  "sympy==1.14.0",
+  "psutil==7.2.2",
+  "scipy==1.17.0",
+  "requests==2.32.5",
+]
+
+[project.optional-dependencies]
+cuda = [
+    "flash-attn>=2.8.3",
 ]
 
 [build-system]
@@ -29,8 +40,19 @@
   "ipywidgets>=8.1.8",
 ]
 
+[tool.uv.sources]
+torch       = { index = "pytorch-cpu" }
+torchvision = { index = "pytorch-cpu" }
+torchaudio  = { index = "pytorch-cpu" }
+
+
+[[tool.uv.index]]
+name = "pytorch-cpu"
+url = "https://download.pytorch.org/whl/cpu"
+explicit = true # Ensures uv only uses this specific index for the configured packages
+
 [tool.uv]
 no-build-isolation-package = ["flash-attn"]
 
 [tool.ruff]
-line-length = 100
+line-length = 100
\ No newline at end of file
diff -ruN alpamayo/src/alpamayo_r1/test_inference.py alpamayo_2/src/alpamayo_r1/test_inference.py
--- alpamayo/src/alpamayo_r1/test_inference.py	2026-02-10 19:25:46.054963600 +0800
+++ alpamayo_2/src/alpamayo_r1/test_inference.py	2026-02-10 19:56:48.615748700 +0800
@@ -18,13 +18,14 @@
 # It can be used to test the inference pipeline.
 
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import numpy as np
 
 from alpamayo_r1.models.alpamayo_r1 import AlpamayoR1
 from alpamayo_r1.load_physical_aiavdataset import load_physical_aiavdataset
 from alpamayo_r1 import helper
 
-
 # Example clip ID
 clip_id = "030c760c-ae38-49aa-9ad8-f5650a545d26"
 print(f"Loading dataset for clip_id: {clip_id}...")
@@ -32,7 +33,9 @@
 print("Dataset loaded.")
 messages = helper.create_message(data["image_frames"].flatten(0, 1))
 
-model = AlpamayoR1.from_pretrained("nvidia/Alpamayo-R1-10B", dtype=torch.bfloat16).to("cuda")
+# use local checkpoints
+local_model_path = "Path of the checkpoint of AR-1 downloaded"
+model = AlpamayoR1.from_pretrained(local_model_path, dtype=torch.bfloat16).to("cuda")
 processor = helper.get_processor(model.tokenizer)
 
 inputs = processor.apply_chat_template(
